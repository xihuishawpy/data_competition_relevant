{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、导入库、数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve, accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, KBinsDiscretizer, LabelEncoder, MinMaxScaler, PowerTransformer\n",
    "\n",
    "print(xgb.__version__)\n",
    "print(lgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:36:50,611 : INFO : data loading...\n"
     ]
    }
   ],
   "source": [
    "logging.info('data loading...')\n",
    "train = pd.read_csv('../xfdata/车辆贷款违约预测数据集/train.csv')\n",
    "test = pd.read_csv('../xfdata/车辆贷款违约预测数据集/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_new_feats(train, test):\n",
    "    '''生成新特征：如年利率/分箱等特征'''\n",
    "    # Step 1: 合并训练集和测试集\n",
    "    data = pd.concat([train, test])\n",
    "\n",
    "    # Step 2: 具体特征工程\n",
    "    # 计算二级账户的年利率\n",
    "    data['sub_Rate'] = (data['sub_account_monthly_payment'] * data['sub_account_tenure'] - data[\n",
    "        'sub_account_sanction_loan']) / data['sub_account_sanction_loan']\n",
    "\n",
    "    # 计算主账户的年利率\n",
    "    data['main_Rate'] = (data['main_account_monthly_payment'] * data['main_account_tenure'] - data[\n",
    "        'main_account_sanction_loan']) / data['main_account_sanction_loan']\n",
    "\n",
    "    # 对部分特征进行分箱操作\n",
    "    # 等宽分箱\n",
    "    loan_to_asset_ratio_labels = [i for i in range(10)]\n",
    "    data['loan_to_asset_ratio_bin'] = pd.cut(data[\"loan_to_asset_ratio\"], 10, labels=loan_to_asset_ratio_labels)\n",
    "    # 等频分箱\n",
    "    data['asset_cost_bin'] = pd.qcut(data['asset_cost'], 10, labels=loan_to_asset_ratio_labels)\n",
    "    # 自定义分箱\n",
    "    amount_cols = [\n",
    "                   'total_monthly_payment',\n",
    "                   'main_account_sanction_loan',\n",
    "                   'main_account_disbursed_loan',\n",
    "                   'sub_account_sanction_loan',\n",
    "                   'sub_account_disbursed_loan',\n",
    "                   'main_account_monthly_payment',\n",
    "                   'sub_account_monthly_payment',\n",
    "                   'total_sanction_loan'\n",
    "                ]\n",
    "    amount_labels = [i for i in range(10)]\n",
    "    for col in amount_cols:\n",
    "        total_monthly_payment_bin = [-1, 5000, 10000, 30000, 50000, 100000, 300000, 500000, 1000000, 3000000, data[col].max()]\n",
    "        data[col + '_bin'] = pd.cut(data[col], total_monthly_payment_bin, labels=amount_labels).astype(int)\n",
    "\n",
    "    # Step 3: 返回包含新特征的训练集 & 测试集\n",
    "    return data[data['loan_default'].notnull()], data[data['loan_default'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、target 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_target_encoding_feats(train, test, encode_cols, target_col, n_fold=10):\n",
    "    '''生成target encoding特征'''\n",
    "    # for training set - cv\n",
    "    tg_feats = np.zeros((train.shape[0], len(encode_cols)))\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, random_state=1024, shuffle=True)\n",
    "    for _, (train_index, val_index) in enumerate(kfold.split(train[encode_cols], train[target_col])):\n",
    "        df_train, df_val = train.iloc[train_index], train.iloc[val_index]\n",
    "        for idx, col in enumerate(encode_cols):\n",
    "            target_mean_dict = df_train.groupby(col)[target_col].mean()\n",
    "            df_val[f'{col}_mean_target'] = df_val[col].map(target_mean_dict)\n",
    "            tg_feats[val_index, idx] = df_val[f'{col}_mean_target'].values\n",
    "\n",
    "    for idx, encode_col in enumerate(encode_cols):\n",
    "        train[f'{encode_col}_mean_target'] = tg_feats[:, idx]\n",
    "\n",
    "    # for testing set\n",
    "    for col in encode_cols:\n",
    "        target_mean_dict = train.groupby(col)[target_col].mean()\n",
    "        test[f'{col}_mean_target'] = test[col].map(target_mean_dict)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、根据id，取近邻的结果概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_neighbor_feats(train, test):\n",
    "    '''产生近邻欺诈特征'''\n",
    "    if not os.path.exists('../user_data/neighbor_default_probs.pkl'):\n",
    "        # 该特征需要跑的时间较久，因此将其存成了pkl文件\n",
    "        neighbor_default_probs = []\n",
    "        for i in tqdm(range(train.customer_id.max())):\n",
    "            if i >= 10 and i < 199706:\n",
    "                customer_id_neighbors = list(range(i - 10, i)) + list(range(i + 1, i + 10))\n",
    "            elif i < 199706:\n",
    "                customer_id_neighbors = list(range(0, i)) + list(range(i + 1, i + 10))\n",
    "            else:\n",
    "                customer_id_neighbors = list(range(i - 10, i)) + list(range(i + 1, 199706))\n",
    "\n",
    "            customer_id_neighbors = [customer_id_neighbor for customer_id_neighbor in customer_id_neighbors if\n",
    "                                     customer_id_neighbor in train.customer_id.values.tolist()]\n",
    "            neighbor_default_prob = train.set_index('customer_id').loc[customer_id_neighbors].loan_default.mean()\n",
    "            neighbor_default_probs.append(neighbor_default_prob)\n",
    "\n",
    "        df_neighbor_default_prob = pd.DataFrame({'customer_id': range(0, train.customer_id.max()),\n",
    "                                                 'neighbor_default_prob': neighbor_default_probs})\n",
    "        save_pkl(df_neighbor_default_prob, '../user_data/neighbor_default_probs.pkl')\n",
    "    else:\n",
    "        df_neighbor_default_prob = load_pkl('../user_data/neighbor_default_probs.pkl')\n",
    "    train = pd.merge(left=train, right=df_neighbor_default_prob, on='customer_id', how='left')\n",
    "    test = pd.merge(left=test, right=df_neighbor_default_prob, on='customer_id', how='left')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:36:51,259 : INFO : feature generating...\n",
      "<ipython-input-6-2b616bc63c9b>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val[f'{col}_mean_target'] = df_val[col].map(target_mean_dict)\n"
     ]
    }
   ],
   "source": [
    "TARGET_ENCODING_FETAS = [\n",
    "                            'employment_type',\n",
    "                             'branch_id',\n",
    "                             'supplier_id',\n",
    "                             'manufacturer_id',\n",
    "                             'area_id',\n",
    "                             'employee_code_id',\n",
    "                             'asset_cost_bin'\n",
    "                         ]\n",
    "\n",
    "\n",
    "# 特征工程\n",
    "logging.info('feature generating...')\n",
    "train, test = gen_new_feats(train, test)\n",
    "train, test = gen_target_encoding_feats(train, test, TARGET_ENCODING_FETAS, target_col='loan_default', n_fold=10)\n",
    "train, test = gen_neighbor_feats(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:36:52,398 : INFO : new data saving...\n"
     ]
    }
   ],
   "source": [
    "SAVE_FEATS = [\n",
    "                 'customer_id',\n",
    "                 'neighbor_default_prob',\n",
    "                 'disbursed_amount',\n",
    "                 'asset_cost',\n",
    "                 'branch_id',\n",
    "                 'supplier_id',\n",
    "                 'manufacturer_id',\n",
    "                 'area_id',\n",
    "                 'employee_code_id',\n",
    "                 'credit_score',\n",
    "                 'loan_to_asset_ratio',\n",
    "                 'year_of_birth',\n",
    "                 'age',\n",
    "                 'sub_Rate',\n",
    "                 'main_Rate',\n",
    "                 'loan_to_asset_ratio_bin',\n",
    "                 'asset_cost_bin',\n",
    "                 'employment_type_mean_target',\n",
    "                 'branch_id_mean_target',\n",
    "                 'supplier_id_mean_target',\n",
    "                 'manufacturer_id_mean_target',\n",
    "                 'area_id_mean_target',\n",
    "                 'employee_code_id_mean_target',\n",
    "                 'asset_cost_bin_mean_target',\n",
    "                 'credit_history',\n",
    "                 'average_age',\n",
    "                 'total_disbursed_loan',\n",
    "                 'main_account_disbursed_loan',\n",
    "                 'total_sanction_loan',\n",
    "                 'main_account_sanction_loan',\n",
    "                 'active_to_inactive_act_ratio',\n",
    "                 'total_outstanding_loan',\n",
    "                 'main_account_outstanding_loan',\n",
    "                 'Credit_level',\n",
    "                 'outstanding_disburse_ratio',\n",
    "                 'total_account_loan_no',\n",
    "                 'main_account_tenure',\n",
    "                 'main_account_loan_no',\n",
    "                 'main_account_monthly_payment',\n",
    "                 'total_monthly_payment',\n",
    "                 'main_account_active_loan_no',\n",
    "                 'main_account_inactive_loan_no',\n",
    "                 'sub_account_inactive_loan_no',\n",
    "                 'enquirie_no',\n",
    "                 'main_account_overdue_no',\n",
    "                 'total_overdue_no',\n",
    "                 'last_six_month_defaulted_no'\n",
    "            ]\n",
    "\n",
    "\n",
    "# 特征工程 一些后处理\n",
    "for col in ['sub_Rate', 'main_Rate', 'outstanding_disburse_ratio']:\n",
    "     train[col] = train[col].apply(lambda x: 1 if x > 1 else x)\n",
    "     test[col] = test[col].apply(lambda x: 1 if x > 1 else x)\n",
    "train['asset_cost_bin'] = train['asset_cost_bin'].astype(int)\n",
    "test['asset_cost_bin'] = test['asset_cost_bin'].astype(int)\n",
    "train['loan_to_asset_ratio_bin'] = train['loan_to_asset_ratio_bin'].astype(int)\n",
    "test['loan_to_asset_ratio_bin'] = test['loan_to_asset_ratio_bin'].astype(int)\n",
    "\n",
    "# 存储包含新特征的数据集\n",
    "logging.info('new data saving...')\n",
    "cols = SAVE_FEATS + ['loan_default', ]\n",
    "train[cols].to_csv('./train_final.csv', index=False)\n",
    "test[cols].to_csv('./test_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、模型训练-交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_kfold(X_train, y_train, X_test, n_fold=5):\n",
    "    '''train lightgbm with k-fold split'''\n",
    "    gbms = []\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, random_state=1024, shuffle=True)\n",
    "    oof_preds = np.zeros((X_train.shape[0],))\n",
    "    test_preds = np.zeros((X_test.shape[0],))\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "        logging.info(f'############ fold {fold} ###########')\n",
    "        X_tr, X_val, y_tr, y_val = X_train.iloc[train_index], X_train.iloc[val_index], y_train[train_index], y_train[val_index]\n",
    "        dtrain = lgb.Dataset(X_tr, y_tr)\n",
    "        dvalid = lgb.Dataset(X_val, y_val, reference=dtrain)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'num_leaves': 64,\n",
    "            'learning_rate': 0.02,\n",
    "            'min_data_in_leaf': 150,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'n_jobs': -1,\n",
    "            'seed': 1024\n",
    "        }\n",
    "\n",
    "        gbm = lgb.train(params,\n",
    "                        dtrain,\n",
    "                        num_boost_round=100,\n",
    "                        valid_sets=[dtrain, dvalid],\n",
    "                        verbose_eval=50,\n",
    "                        early_stopping_rounds=20)\n",
    "\n",
    "        oof_preds[val_index] = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n",
    "        test_preds += gbm.predict(X_test, num_iteration=gbm.best_iteration) / kfold.n_splits\n",
    "        gbms.append(gbm)\n",
    "\n",
    "    return gbms, oof_preds, test_preds\n",
    "\n",
    "\n",
    "\n",
    "def train_xgb_kfold(X_train, y_train, X_test, n_fold=5):\n",
    "    '''train xgboost with k-fold split'''\n",
    "    gbms = []\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1024, shuffle=True)\n",
    "    oof_preds = np.zeros((X_train.shape[0],))\n",
    "    test_preds = np.zeros((X_test.shape[0],))\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "        logging.info(f'############ fold {fold} ###########')\n",
    "        X_tr, X_val, y_tr, y_val = X_train.iloc[train_index], X_train.iloc[val_index], y_train[train_index], y_train[val_index]\n",
    "        dtrain = xgb.DMatrix(X_tr, y_tr)\n",
    "        dvalid = xgb.DMatrix(X_val, y_val)\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "        params={\n",
    "            'booster':'gbtree',\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': ['logloss', 'auc'],\n",
    "            'max_depth': 8,\n",
    "            'subsample':0.9,\n",
    "            'min_child_weight': 10,\n",
    "            'colsample_bytree':0.85,\n",
    "            'lambda': 10,\n",
    "            'eta': 0.02,\n",
    "            'seed': 1024\n",
    "        }\n",
    "\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'test')]\n",
    "\n",
    "        gbm = xgb.train(params,\n",
    "                        dtrain,\n",
    "                        num_boost_round=100,\n",
    "                        evals=watchlist,\n",
    "                        verbose_eval=50,\n",
    "                        early_stopping_rounds=20)\n",
    "\n",
    "        oof_preds[val_index] = gbm.predict(dvalid, iteration_range=(0, gbm.best_iteration))\n",
    "        test_preds += gbm.predict(dtest, iteration_range=(0, gbm.best_iteration)) / kfold.n_splits\n",
    "        gbms.append(gbm)\n",
    "\n",
    "    return gbms, oof_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、模型训练-结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(train, test, feat_cols, label_col, n_fold=10):\n",
    "    '''训练xgboost'''\n",
    "    for col in ['sub_Rate', 'main_Rate', 'outstanding_disburse_ratio']:\n",
    "        train[col] = train[col].apply(lambda x: 1 if x > 1 else x)\n",
    "        test[col] = test[col].apply(lambda x: 1 if x > 1 else x)\n",
    "\n",
    "    X_train = train[feat_cols]\n",
    "    y_train = train[label_col]\n",
    "    X_test = test[feat_cols]\n",
    "    gbms_xgb, oof_preds_xgb, test_preds_xgb = train_xgb_kfold(X_train, y_train, X_test, n_fold=n_fold)\n",
    "\n",
    "    if not os.path.exists('../user_data/gbms_xgb.pkl'):\n",
    "        save_pkl(gbms_xgb, '../user_data/gbms_xgb.pkl')\n",
    "\n",
    "    return gbms_xgb, oof_preds_xgb, test_preds_xgb\n",
    "\n",
    "\n",
    "def train_lgb(train, test, feat_cols, label_col, n_fold=10):\n",
    "    '''训练lightgbm'''\n",
    "    X_train = train[feat_cols]\n",
    "    y_train = train[label_col]\n",
    "    X_test = test[feat_cols]\n",
    "    gbms_lgb, oof_preds_lgb, test_preds_lgb = train_lgb_kfold(X_train, y_train, X_test, n_fold=n_fold)\n",
    "\n",
    "    if not os.path.exists('../user_data/gbms_lgb.pkl'):\n",
    "        save_pkl(gbms_lgb, '../user_data/gbms_lgb.pkl')\n",
    "\n",
    "    return gbms_lgb, oof_preds_lgb, test_preds_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:37:37,711 : INFO : data loading...\n",
      "2021-12-08 14:37:38,197 : INFO : feature generating...\n",
      "<ipython-input-6-2b616bc63c9b>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val[f'{col}_mean_target'] = df_val[col].map(target_mean_dict)\n",
      "2021-12-08 14:37:39,540 : INFO : ############ fold 0 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68452\ttrain-auc:0.63921\ttest-logloss:0.68456\ttest-auc:0.62855\n",
      "[50]\ttrain-logloss:0.48456\ttrain-auc:0.69437\ttest-logloss:0.48960\ttest-auc:0.65455\n",
      "[99]\ttrain-logloss:0.44098\ttrain-auc:0.71345\ttest-logloss:0.45205\ttest-auc:0.66009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:37:48,938 : INFO : ############ fold 1 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68450\ttrain-auc:0.63775\ttest-logloss:0.68462\ttest-auc:0.62587\n",
      "[50]\ttrain-logloss:0.48443\ttrain-auc:0.69538\ttest-logloss:0.49083\ttest-auc:0.65386\n",
      "[99]\ttrain-logloss:0.44087\ttrain-auc:0.71447\ttest-logloss:0.45330\ttest-auc:0.65865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:37:58,284 : INFO : ############ fold 2 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68451\ttrain-auc:0.64048\ttest-logloss:0.68455\ttest-auc:0.62541\n",
      "[50]\ttrain-logloss:0.48439\ttrain-auc:0.69509\ttest-logloss:0.49013\ttest-auc:0.65240\n",
      "[99]\ttrain-logloss:0.44086\ttrain-auc:0.71396\ttest-logloss:0.45280\ttest-auc:0.65747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:38:07,753 : INFO : ############ fold 3 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68452\ttrain-auc:0.63414\ttest-logloss:0.68465\ttest-auc:0.62270\n",
      "[50]\ttrain-logloss:0.48450\ttrain-auc:0.69570\ttest-logloss:0.48949\ttest-auc:0.66355\n",
      "[99]\ttrain-logloss:0.44089\ttrain-auc:0.71486\ttest-logloss:0.45175\ttest-auc:0.66649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:38:17,367 : INFO : ############ fold 4 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68452\ttrain-auc:0.63459\ttest-logloss:0.68455\ttest-auc:0.63408\n",
      "[50]\ttrain-logloss:0.48469\ttrain-auc:0.69490\ttest-logloss:0.48868\ttest-auc:0.66670\n",
      "[99]\ttrain-logloss:0.44128\ttrain-auc:0.71371\ttest-logloss:0.45032\ttest-auc:0.67131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:38:26,926 : INFO : ############ fold 5 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68450\ttrain-auc:0.64065\ttest-logloss:0.68462\ttest-auc:0.61957\n",
      "[50]\ttrain-logloss:0.48435\ttrain-auc:0.69548\ttest-logloss:0.49044\ttest-auc:0.65277\n",
      "[99]\ttrain-logloss:0.44071\ttrain-auc:0.71474\ttest-logloss:0.45295\ttest-auc:0.65920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:38:36,512 : INFO : ############ fold 6 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68448\ttrain-auc:0.63988\ttest-logloss:0.68464\ttest-auc:0.61795\n",
      "[50]\ttrain-logloss:0.48424\ttrain-auc:0.69561\ttest-logloss:0.49100\ttest-auc:0.64600\n",
      "[99]\ttrain-logloss:0.44052\ttrain-auc:0.71490\ttest-logloss:0.45404\ttest-auc:0.65147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:38:46,023 : INFO : ############ fold 7 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68453\ttrain-auc:0.63641\ttest-logloss:0.68457\ttest-auc:0.63026\n",
      "[50]\ttrain-logloss:0.48457\ttrain-auc:0.69379\ttest-logloss:0.48922\ttest-auc:0.65895\n",
      "[99]\ttrain-logloss:0.44103\ttrain-auc:0.71357\ttest-logloss:0.45125\ttest-auc:0.66454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:38:55,479 : INFO : ############ fold 8 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68449\ttrain-auc:0.63646\ttest-logloss:0.68453\ttest-auc:0.62626\n",
      "[50]\ttrain-logloss:0.48458\ttrain-auc:0.69478\ttest-logloss:0.48932\ttest-auc:0.65600\n",
      "[99]\ttrain-logloss:0.44093\ttrain-auc:0.71402\ttest-logloss:0.45145\ttest-auc:0.66282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:05,081 : INFO : ############ fold 9 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68452\ttrain-auc:0.63518\ttest-logloss:0.68459\ttest-auc:0.62040\n",
      "[50]\ttrain-logloss:0.48459\ttrain-auc:0.69417\ttest-logloss:0.49000\ttest-auc:0.65480\n",
      "[99]\ttrain-logloss:0.44099\ttrain-auc:0.71341\ttest-logloss:0.45241\ttest-auc:0.66107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:15,159 : INFO : ############ fold 0 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 23891, number of negative: 111109\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7066\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176970 -> initscore=-1.537010\n",
      "[LightGBM] [Info] Start training from score -1.537010\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.680728\tvalid_1's auc: 0.657064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:17,237 : INFO : ############ fold 1 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.695359\tvalid_1's auc: 0.663392\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.695359\tvalid_1's auc: 0.663392\n",
      "[LightGBM] [Info] Number of positive: 23891, number of negative: 111109\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7059\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176970 -> initscore=-1.537010\n",
      "[LightGBM] [Info] Start training from score -1.537010\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.681283\tvalid_1's auc: 0.653969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:19,523 : INFO : ############ fold 2 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.696122\tvalid_1's auc: 0.659682\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.696122\tvalid_1's auc: 0.659682\n",
      "[LightGBM] [Info] Number of positive: 23891, number of negative: 111109\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7071\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176970 -> initscore=-1.537010\n",
      "[LightGBM] [Info] Start training from score -1.537010\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.681236\tvalid_1's auc: 0.65142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:21,286 : INFO : ############ fold 3 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.695948\tvalid_1's auc: 0.658272\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.695948\tvalid_1's auc: 0.658272\n",
      "[LightGBM] [Info] Number of positive: 23891, number of negative: 111109\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7065\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176970 -> initscore=-1.537010\n",
      "[LightGBM] [Info] Start training from score -1.537010\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.680678\tvalid_1's auc: 0.66442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:23,257 : INFO : ############ fold 4 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.695676\tvalid_1's auc: 0.668496\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.695676\tvalid_1's auc: 0.668496\n",
      "[LightGBM] [Info] Number of positive: 23891, number of negative: 111109\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7072\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176970 -> initscore=-1.537010\n",
      "[LightGBM] [Info] Start training from score -1.537010\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.680246\tvalid_1's auc: 0.6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:25,504 : INFO : ############ fold 5 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.695177\tvalid_1's auc: 0.672279\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.695177\tvalid_1's auc: 0.672279\n",
      "[LightGBM] [Info] Number of positive: 23890, number of negative: 111110\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7068\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176963 -> initscore=-1.537061\n",
      "[LightGBM] [Info] Start training from score -1.537061\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.680389\tvalid_1's auc: 0.654878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:27,457 : INFO : ############ fold 6 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.695173\tvalid_1's auc: 0.662115\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.695173\tvalid_1's auc: 0.662115\n",
      "[LightGBM] [Info] Number of positive: 23890, number of negative: 111110\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7066\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176963 -> initscore=-1.537061\n",
      "[LightGBM] [Info] Start training from score -1.537061\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.682069\tvalid_1's auc: 0.647487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:29,387 : INFO : ############ fold 7 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.696339\tvalid_1's auc: 0.653793\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.696339\tvalid_1's auc: 0.653793\n",
      "[LightGBM] [Info] Number of positive: 23890, number of negative: 111110\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7071\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176963 -> initscore=-1.537061\n",
      "[LightGBM] [Info] Start training from score -1.537061\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.680425\tvalid_1's auc: 0.661555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:31,464 : INFO : ############ fold 8 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.695188\tvalid_1's auc: 0.667369\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.695188\tvalid_1's auc: 0.667369\n",
      "[LightGBM] [Info] Number of positive: 23890, number of negative: 111110\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7073\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176963 -> initscore=-1.537061\n",
      "[LightGBM] [Info] Start training from score -1.537061\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.6806\tvalid_1's auc: 0.658909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 14:39:33,538 : INFO : ############ fold 9 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.695282\tvalid_1's auc: 0.665429\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.695282\tvalid_1's auc: 0.665429\n",
      "[LightGBM] [Info] Number of positive: 23890, number of negative: 111110\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7070\n",
      "[LightGBM] [Info] Number of data points in the train set: 135000, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176963 -> initscore=-1.537061\n",
      "[LightGBM] [Info] Start training from score -1.537061\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.681306\tvalid_1's auc: 0.653703\n",
      "[100]\ttraining's auc: 0.695862\tvalid_1's auc: 0.659826\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.695862\tvalid_1's auc: 0.659826\n"
     ]
    }
   ],
   "source": [
    "# 读取原始数据集\n",
    "logging.info('data loading...')\n",
    "train = pd.read_csv('../xfdata/车辆贷款违约预测数据集/train.csv')\n",
    "test = pd.read_csv('../xfdata/车辆贷款违约预测数据集/test.csv')\n",
    "\n",
    "# 特征工程\n",
    "logging.info('feature generating...')\n",
    "train, test = gen_new_feats(train, test)\n",
    "train, test = gen_target_encoding_feats(train, test, TARGET_ENCODING_FETAS, target_col='loan_default', n_fold=10)\n",
    "train, test = gen_neighbor_feats(train, test)\n",
    "\n",
    "train['asset_cost_bin'] = train['asset_cost_bin'].astype(int)\n",
    "test['asset_cost_bin'] = test['asset_cost_bin'].astype(int)\n",
    "train['loan_to_asset_ratio_bin'] = train['loan_to_asset_ratio_bin'].astype(int)\n",
    "test['loan_to_asset_ratio_bin'] = test['loan_to_asset_ratio_bin'].astype(int)\n",
    "train['asset_cost_bin_mean_target'] = train['asset_cost_bin_mean_target'].astype(float)\n",
    "test['asset_cost_bin_mean_target'] = test['asset_cost_bin_mean_target'].astype(float)\n",
    "\n",
    "# 模型训练：linux和mac的xgboost结果会有些许不同，以模型文件结果为主\n",
    "gbms_xgb, oof_preds_xgb, test_preds_xgb = train_xgb(train.copy(), test.copy(),\n",
    "                                                    feat_cols=SAVE_FEATS,\n",
    "                                                    label_col='loan_default')\n",
    "gbms_lgb, oof_preds_lgb, test_preds_lgb = train_lgb(train, test,\n",
    "                                                    feat_cols=SAVE_FEATS,\n",
    "                                                    label_col='loan_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、预测结果阈值划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_thres_new(df_train, oof_preds):\n",
    "    df_train['oof_preds'] = oof_preds\n",
    "    quantile_point = df_train['loan_default'].mean() # 可看作训练集取到loan_default=1的概率\n",
    "    thres = df_train['oof_preds'].quantile(1 - quantile_point) # 比如 0,1,1,1 mean=0.75 1-mean=0.25,也就是25%分位数取值为0\n",
    "\n",
    "    _thresh = []\n",
    "    for thres_item in np.arange(thres - 0.2, thres + 0.2, 0.01): #  按照理论阈值的上下0.2范围，0.01步长，找到最佳阈值，f1分数最高对应的阈值即为最佳阈值\n",
    "        _thresh.append(\n",
    "            [thres_item, f1_score(df_train['loan_default'], np.where(oof_preds > thres_item, 1, 0), average='macro')])\n",
    "\n",
    "    _thresh = np.array(_thresh)\n",
    "    best_id = _thresh[:, 1].argmax() # 找到f1最高对应的行\n",
    "    best_thresh = _thresh[best_id][0] # 取出最佳阈值\n",
    "\n",
    "    print(\"阈值: {}\\n训练集的f1: {}\".format(best_thresh, _thresh[best_id][1]))\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阈值: 0.2728475863923628\n",
      "训练集的f1: 0.5831012392678149\n",
      "阈值: 0.2277117872193553\n",
      "训练集的f1: 0.5841566537815013\n",
      "阈值: 0.8110024552466668\n",
      "训练集的f1: 0.5849155835510944\n"
     ]
    }
   ],
   "source": [
    "xgb_thres = gen_thres_new(train, oof_preds_xgb)\n",
    "lgb_thres =  gen_thres_new(train, oof_preds_lgb)\n",
    "\n",
    "\n",
    "# 结果聚合\n",
    "df_oof_res = pd.DataFrame({'customer_id': train['customer_id'],\n",
    "                            'loan_default':train['loan_default'],\n",
    "                            'oof_preds_xgb': oof_preds_xgb,\n",
    "                            'oof_preds_lgb': oof_preds_lgb})\n",
    "\n",
    "# 模型融合\n",
    "df_oof_res['xgb_rank'] = df_oof_res['oof_preds_xgb'].rank(pct=True) # percentile rank,返回的是排序后的分位数\n",
    "df_oof_res['lgb_rank'] = df_oof_res['oof_preds_lgb'].rank(pct=True)\n",
    "\n",
    "df_oof_res['preds'] = 0.31 * df_oof_res['xgb_rank'] + 0.69 * df_oof_res['lgb_rank']\n",
    "\n",
    "\n",
    "thres = gen_thres_new(df_oof_res, df_oof_res['preds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8、测试集结果阈值划分，输出最终预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving result to: ../prediction_result/result.csv\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gen_submit_file(df_test, test_preds, thres, save_path):\n",
    "    df_test['test_preds_binary'] = np.where(test_preds > thres, 1, 0)  # 按最终模型融合后的阈值进行划分\n",
    "    df_test_submit = df_test[['customer_id', 'test_preds_binary']]\n",
    "    df_test_submit.columns = ['customer_id', 'loan_default']\n",
    "    print(f'saving result to: {save_path}')\n",
    "    df_test_submit.to_csv(save_path, index=False)\n",
    "    print('done!')\n",
    "    return df_test_submit\n",
    "\n",
    "\n",
    "\n",
    "df_test_res = pd.DataFrame({'customer_id': test['customer_id'],\n",
    "                                'test_preds_xgb': test_preds_xgb,\n",
    "                                'test_preds_lgb': test_preds_lgb})\n",
    "\n",
    "df_test_res['xgb_rank'] = df_test_res['test_preds_xgb'].rank(pct=True)\n",
    "df_test_res['lgb_rank'] = df_test_res['test_preds_lgb'].rank(pct=True)\n",
    "df_test_res['preds'] = 0.31 * df_test_res['xgb_rank'] + 0.69 * df_test_res['lgb_rank']\n",
    "\n",
    "# 结果产出\n",
    "df_submit = gen_submit_file(df_test_res, df_test_res['preds'], thres,\n",
    "                            save_path='../prediction_result/result.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d86c190dfcadcdaa67edec4a1ea82702241987b5b1f320c920d3d4ca36fee5b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
